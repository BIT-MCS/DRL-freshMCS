
class ActingInterface():
    """The base acting interface any actor or actor pool needs to implement"""

    # Observation and action spaces for the environment, should be initialized
    # during init after ENV creations
    _observation_space = None
    _action_space = None

    def __init__(self, observation_space, action_space):
        self._observation_space = observation_space
        self._action_space = action_space

    def get_spaces(self):
        """Returns the observation and action space for the ENV used"""
        return self._observation_space, self._action_space

    def get_samples(self, min_samples):
        """Gets new samples from the actors

        Returns at least 'min_samples' (Blocks if not available), or whatever
        is available if there are more.

        Returns: A list of single-env samples in the format returned by
            '_create_sample()'
        """
        raise NotImplementedError

    def get_env_count(self):
        """Returns the total amount of ENV instances running in this actor
        configuration
        """
        raise NotImplementedError

    def set_actor_policy(self, actor_policy):
        """Sets the policy to use for acting"""
        raise NotImplementedError

    def update_state(self, progress, policy_state=None):
        """Updates training/policy-state for the actors

        Args:
            progess: Progress of the training (In case for example of some
                exploration rate which relies on it), in [0,1] range
            policy_state: Optional updated policy state to load to the
                acting policy (In particular the weights)
        """
        raise NotImplementedError

    def close(self):
        """Closes the acting gracefully

        In particular should close all ENVs gracefully and any actor resources
        such as sub-processes, threads etc...
        """
        raise NotImplementedError

    def _create_sample(self, policy_output, next_state, reward, done, info,
                       env_id):
        """Called by the actor to create a new (single-env) acted sample
        in the format the training/history buffers expect to get

        Args:
            policy_output: The output dictionary from the policy when doing
                action-selection, in particular includes the selected actions
            next_state: The subsequent policy input state generated by taking
                the selected action (Includes in particular the new env
                observation and additional policy input states such as RNN
                hidden states)
            reward: The reward received from the ENV when taking the action
            done: The done indication received fom the ENV when taking the
                action
            info: The 'info' dictionary received from the ENV when taking the
                action
            env_id: A unique ID for the ENV. Can be int/str or any hashable
                type. Must be unique across ALL acting ENVs participating in
                the training session

        Note: To save space/transfer-time we do not include the initial/current
        state which generated this transition/output (This is available as the
        'next_state' of the previous timestep for the specific env_id)
        """
        return {
            "policy_output": policy_output,
            "next_state": next_state,
            "reward": reward,
            "done": done,
            "info": info,
            "env_id": env_id
        }
